{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    " \n",
    "#Usaremos o pandas para algumas transformações basicas\n",
    "import pandas as pd\n",
    " \n",
    "#Numpy apenas para deixar as coisas mais rapidas\n",
    "import numpy as np\n",
    " \n",
    "#Irei utilizar essa função do sklearn para dividir o conjunto de treino e teste\n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "#As metricas básica acuracia e confusio_matrix\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    " \n",
    "#Usamos pandas para abrir o dataset\n",
    "data = pd.read_csv(\"twitter.csv\",encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos essa função para normalizar o coluna Species\n",
    "#Exemplo [preto,branco,preto,azul] => [0,1,0,2]\n",
    "#def normalizeIris(x):\n",
    "#    if x == \"virginica\":\n",
    "#        return 0\n",
    "#    elif x == \"versicolor\":\n",
    "#        return 1\n",
    "#    else:\n",
    "#        return 2\n",
    " \n",
    "#fazemos a função para transformar as labels em one-hot vectors\n",
    "#Exemplo [1,2,1,0] => [[0,1,0],[0,0,1],[0,1,0],[1,0,0]]\n",
    "def makeHotvector(y_data):\n",
    "    labels = []\n",
    "    for x in range(len(y_data)):\n",
    "        labels.append([0,0,0])\n",
    "        labels[x][y_data[x]] = 1\n",
    "    y_data = np.array(labels,dtype=np.float64)\n",
    "    return y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usamos o método .apply do Pandas para aplicar a função normaLizeIris\n",
    "#Na coluna \"Species\" lembre o que essa função faz\n",
    "#data[\"Species\"] = data[\"Species\"].apply(normalizeIris)\n",
    " \n",
    "#Aqui separamos as features em uma variavel e as labels em outra\n",
    "x_data = data.drop(\"Sentiment\",axis=1).values\n",
    "y_data = data[\"Sentiment\"].values\n",
    " \n",
    "#Aqui separamos um pouco do dataset para treino e a outra parte para teste\n",
    "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A quantidade inputs na rede\n",
    "n_input = 4\n",
    "#A proporção em que a rede irá atualizar cada peso\n",
    "#Chamamos também de alpha\n",
    "learning_rate = 0.01\n",
    "# A quantidade de unidades no primeiro layer\n",
    "n_hidden_unites_1 = 5\n",
    "#A quantidade de unidades no segundo layer\n",
    "n_hidden_unites_2 = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos criar o grapho da rede\n",
    "#Primeiro definimos a sessão\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aqui são os inputs da rede\n",
    "#Apenas os inputs do dataset são considerados.\n",
    "X = tf.placeholder(shape=[None,n_input],dtype=tf.float64)\n",
    " \n",
    "#Aqui ficam as Labels para treino\n",
    "#Essa parte do grafo não aparece no esquema.\n",
    "#mas aqui fica as labels que serão usadas como exemplo para a rede.\n",
    "y_ =  tf.placeholder(dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agora iremos definir os pesos da rede\n",
    "#Esse é nosso primeira camada de pesos.\n",
    "W = {\"h1\":tf.Variable(tf.random_normal([n_input,n_hidden_unites_1],dtype=tf.float64),dtype=tf.float64),\n",
    "     \"h2\":tf.Variable(tf.random_normal([n_hidden_unites_1,n_hidden_unites_2],dtype=tf.float64),dtype=tf.float64)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esse é o bias da rede\n",
    "b = {\"b1\":tf.Variable(tf.random_normal([n_hidden_unites_1],dtype=tf.float64),dtype=tf.float64),\n",
    "     \"b2\":tf.Variable(tf.random_normal([n_hidden_unites_2],dtype=tf.float64),dtype=tf.float64)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sigmoide aplicado a rede\n",
    "#O output do hidden_1\n",
    "out_hidden_1 = tf.nn.sigmoid(tf.matmul(X,W[\"h1\"])+b[\"b1\"])\n",
    "#o output do hidden_2 que alias será o usado como nosso output.\n",
    "out_hidden_2 = tf.nn.sigmoid(tf.matmul(out_hidden_1,W[\"h2\"])+b[\"b2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dark/ner/lib/python3.6/site-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "#Definimos o erro quadratico do out_hidden_2 em relação y_\n",
    "#isso permite deriva-lo e depois ajustar os pesos\n",
    "mse = tf.losses.mean_squared_error(y_,out_hidden_2)\n",
    " \n",
    "#Aqui eu uso o método gradientDescent para otimizar o parametro mse\n",
    "#Ele ira derivar o erro em relação a cada pesso da rede e fazer um atualização\n",
    "#para cima ou para abaixo proporcional ao learning_hate da rede\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uma maneira de carregar as variaveis para a memoria.\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: \"#ASOT awesomeness !!!! can't wait for the power to kick in the night! \"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-e516aa49281e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m#Aqui treinamos a rede uma iteração de cada vez\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ner/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m   2437\u001b[0m         \u001b[0mnone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2438\u001b[0m     \"\"\"\n\u001b[0;32m-> 2439\u001b[0;31m     \u001b[0m_run_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2441\u001b[0m \u001b[0m_gradient_registry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegistry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gradient\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ner/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[0;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   5440\u001b[0m                        \u001b[0;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5441\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 5442\u001b[0;31m   \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ner/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ner/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1147\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[0;32m~/ner/lib/python3.6/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: \"#ASOT awesomeness !!!! can't wait for the power to kick in the night! \""
     ]
    }
   ],
   "source": [
    "#Realizar 1000 interações no dataset\n",
    "for x in range(1000):\n",
    "    print(x)#para ver o andamento do treino\n",
    " \n",
    "    #Iremos dar loop do tamanho do dataset\n",
    "    for _ in range(len(x_train)):\n",
    "        #Aqui seleciono uma instancia aliatória do dataset\n",
    "        i = np.random.randint(len(x_train))\n",
    "        x = np.array([x_train[i]])\n",
    "        y = np.array([y_train[i]])\n",
    " \n",
    "        #Aqui treinamos a rede uma iteração de cada vez\n",
    "        train_step.run(feed_dict={X:x,y_:y},session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
